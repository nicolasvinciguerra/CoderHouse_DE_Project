[2023-12-12T01:23:18.162+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_openaq.create_tables manual__2023-12-12T01:22:14.274918+00:00 [queued]>
[2023-12-12T01:23:18.176+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_openaq.create_tables manual__2023-12-12T01:22:14.274918+00:00 [queued]>
[2023-12-12T01:23:18.177+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 4
[2023-12-12T01:23:18.193+0000] {taskinstance.py:1382} INFO - Executing <Task(PostgresOperator): create_tables> on 2023-12-12 01:22:14.274918+00:00
[2023-12-12T01:23:18.204+0000] {standard_task_runner.py:57} INFO - Started process 461 to run task
[2023-12-12T01:23:18.209+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_openaq', 'create_tables', 'manual__2023-12-12T01:22:14.274918+00:00', '--job-id', '354', '--raw', '--subdir', 'DAGS_FOLDER/dag_openaq.py', '--cfg-path', '/tmp/tmp9_au_cwy']
[2023-12-12T01:23:18.229+0000] {standard_task_runner.py:85} INFO - Job 354: Subtask create_tables
[2023-12-12T01:23:18.321+0000] {task_command.py:416} INFO - Running <TaskInstance: dag_openaq.create_tables manual__2023-12-12T01:22:14.274918+00:00 [running]> on host 63013c88d645
[2023-12-12T01:23:18.407+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dag_openaq' AIRFLOW_CTX_TASK_ID='create_tables' AIRFLOW_CTX_EXECUTION_DATE='2023-12-12T01:22:14.274918+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-12-12T01:22:14.274918+00:00'
[2023-12-12T01:23:18.410+0000] {sql.py:274} INFO - Executing: --STAR SCHEME DIMENSIONAL MODEL - CREATE TABLES
--DROP TABLE IF EXISTS dim_location;
CREATE TABLE IF NOT EXISTS nicolasmvinciguerra_coderhouse.dim_location (
    id INT,
    city VARCHAR(256),
    name VARCHAR(256),
    country_code VARCHAR(2),
    country_name VARCHAR(50),
    sources VARCHAR(256),
    ismobile BOOL,
    isanalysis BOOL,
    sensortype VARCHAR(256),
    coordinates_latitude FLOAT,
    coordinates_longitude FLOAT,
    updated_at timestamp without time zone
    )
DISTSTYLE ALL --es una tabla de tipo SCD no muy extensa, recomendable que se replique en todos los nodos.
SORTKEY (id); --si bien es comun filtrar por ciudad o pais no se incluyen como sortkey ya que el locationid esta correlacionado con estos campos.

--DROP TABLE IF EXISTS dim_parameter;
CREATE TABLE IF NOT EXISTS nicolasmvinciguerra_coderhouse.dim_parameter (
    id INT,
    code VARCHAR(256),
    displayName	VARCHAR(256),
    description	VARCHAR(256),
    preferredUnit VARCHAR(256),
    updated_at timestamp without time zone
    )
DISTSTYLE ALL --es una tabla de tipo SCD no muy extensa, recomendable que se replique en todos los nodos.
SORTKEY (id); --si bien es comun filtrar por el nombre del parametro, su id lo identifica univocamente con lo cual no es necesario incluirlo como sortkey.

--DROP TABLE IF EXISTS fact_measure;
CREATE TABLE IF NOT EXISTS nicolasmvinciguerra_coderhouse.fact_measure (
    location_id INTEGER,
    parameter_code VARCHAR(20),
    value FLOAT,
    date_local TIMESTAMP,
    date_utc TIMESTAMP DISTKEY, --es el campo mas comunmente utilizado para el filtrado
    updated_at timestamp without time zone
    )
SORTKEY (date_utc,location_id,parameter_code);


--STAGING TABLES
DROP TABLE IF EXISTS stg_countries;
CREATE TABLE  nicolasmvinciguerra_coderhouse.stg_countries (
    code VARCHAR(2),
    name VARCHAR(50)
    )
DISTSTYLE EVEN;

DROP TABLE IF EXISTS stg_locations;
CREATE TABLE  nicolasmvinciguerra_coderhouse.stg_locations (
    id INT,
    city VARCHAR(256),
    name VARCHAR(256),
    entity VARCHAR(256),
    country VARCHAR(2),
    sources VARCHAR(256),
    ismobile BOOL,
    isanalysis BOOL,
    sensortype VARCHAR(256),
    lastupdated TIMESTAMP,
    firstupdated TIMESTAMP,
    coordinates_latitude FLOAT,
    coordinates_longitude FLOAT
    )
DISTSTYLE EVEN;

DROP TABLE IF EXISTS stg_parameters;
CREATE TABLE  nicolasmvinciguerra_coderhouse.stg_parameters (
    id INT,
    name VARCHAR(256),
    displayName	VARCHAR(256),
    description	VARCHAR(256),
    preferredUnit VARCHAR(256)
    )
DISTSTYLE EVEN;

DROP TABLE IF EXISTS stg_measurements;
CREATE TABLE  nicolasmvinciguerra_coderhouse.stg_measurements (
    locationId INT,
    location VARCHAR(256),
    parameter VARCHAR(20),
    value FLOAT,
    unit VARCHAR(256),
    country VARCHAR(2),
    city VARCHAR(256),
    isMobile BOOL,
    isAnalysis BOOL,
    entity	VARCHAR(256),
    sensorType VARCHAR(256),
    date_utc TIMESTAMP,
    date_local TIMESTAMP,
    coordinates_latitude FLOAT,
    coordinates_longitude FLOAT
    )
DISTSTYLE EVEN;
[2023-12-12T01:23:18.413+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-12-12T01:23:18.414+0000] {base.py:73} INFO - Using connection ID 'redshift' for task execution.
[2023-12-12T01:23:18.424+0000] {base.py:73} INFO - Using connection ID 'redshift' for task execution.
[2023-12-12T01:23:19.206+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/common/sql/operators/sql.py", line 280, in execute
    output = hook.run(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/common/sql/hooks/sql.py", line 385, in run
    with closing(self.get_conn()) as conn:
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/postgres/hooks/postgres.py", line 155, in get_conn
    self.conn = psycopg2.connect(**conn_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com" (52.44.43.227), port 5439 failed: FATAL:  database "nicolasmvinciguerra_coderhouse" does not exist

[2023-12-12T01:23:19.215+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_openaq, task_id=create_tables, execution_date=20231212T012214, start_date=20231212T012318, end_date=20231212T012319
[2023-12-12T01:23:19.224+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 354 for task create_tables (connection to server at "data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com" (52.44.43.227), port 5439 failed: FATAL:  database "nicolasmvinciguerra_coderhouse" does not exist
; 461)
[2023-12-12T01:23:19.263+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-12-12T01:23:19.280+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
